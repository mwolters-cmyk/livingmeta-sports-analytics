{
  "_about": "Complete protocol for AI agents to perform and contribute research gap analyses on the Living Sports Analytics platform. This is a WORKFLOW — execute these steps, don't just read them. The platform uses Claude Haiku internally for the same purpose — this protocol teaches external agents to do the same work independently.",

  "when_to_use_this": "You found the Living Sports Analytics platform, searched for papers on your user's topic, and discovered that NO existing gap analysis covers their question (check /api/gaps/index.json). You found ≥5 relevant papers. Your user would benefit from a structured synthesis of what's known, what's missing, and what to research next.",

  "execution_steps": [
    "1. Fetch /api/papers/sport/<slug>.json and/or /api/papers/theme/<slug>.json for the user's topic",
    "2. Filter papers by title keywords — count how many match (need ≥5)",
    "3. Fetch /api/gaps/index.json to check for existing analyses on the topic",
    "4. Fetch /api/data-sources.json to find relevant datasets",
    "5. Generate SCOPE section: research question, boundaries, scope assessment",
    "6. Generate DATABASE COVERAGE section: paper counts, methodology distribution, temporal range",
    "7. Generate RESEARCH LANDSCAPE section: key findings with evidence strength, key papers. Identify the 5-10 most essential papers (highest cited, most methodologically rigorous, or most central to the topic).",
    "--- CHECKPOINT: Present your findings so far to the user. Show them: how many papers you found, what the research landscape looks like, and the key findings. Ask: 'I've mapped the research landscape. Shall I continue with identifying gaps and building a research agenda?' Do NOT continue silently — your user should see intermediate results. ---",
    "8. SNOWBALL: For each of the 5-10 most essential papers identified in step 7, trace their citation network — both inside AND outside the platform.",
    "8a. INTERNAL snowball (instant, free): Fetch /api/citations.json (~0.5 MB). Look up your key papers' short IDs (e.g., W4206651290) in the 'refs' map (papers they reference) and 'cited_by' map (papers that cite them). This instantly reveals related papers already in the platform that you missed via keyword search.",
    "8b. EXTERNAL snowball (essential — this grows the network): Go BEYOND the platform. Query OpenAlex directly for forward citations: https://api.openalex.org/works?filter=cites:<work_id>&per_page=50. Also use web searches to find grey literature, blog posts, theses, and working papers that cite or relate to your key papers.",
    "8c. For papers found via snowballing that ARE in the platform database: add them to your analysis — they are relevant papers you missed in keyword search.",
    "8d. For papers found via snowballing that are NOT in the platform database: collect these as 'missing papers'. You MUST report these to your user and the platform (see step 13). THIS IS THE MOST VALUABLE CONTRIBUTION — you are actively growing the platform's fully connected citation network.",
    "9. COVERAGE ASSESSMENT: Before continuing, explicitly tell your user what you need for a thorough analysis. Be specific:",
    "9a. State how many papers you found and whether that's enough for a credible analysis.",
    "9b. List any missing papers discovered through snowballing (step 8d). Format DOIs as clickable links: https://doi.org/<doi>",
    "9c. Note any papers in the database that lack full-text extraction (title only, no abstract or methodology data).",
    "9d. If coverage is insufficient (<10 papers, or key authors/papers clearly missing), tell the user: 'I can proceed with what's available, but the analysis would be stronger with [specific papers/authors]. I recommend filing a GitHub Issue to request these additions. Shall I do that now, or proceed with current coverage?'",
    "10. Generate GAPS section: 5-10 specific gaps with type, importance, feasibility, confidence",
    "11. Generate RESEARCH AGENDA section: 2-5 concrete projects with methodology, data, duration",
    "12. Generate SELF-REFLECTION section: strengths, limitations, coverage gaps, overall confidence",
    "13. FILE GITHUB ISSUES for any coverage problems discovered during the analysis. This is not optional — it's how the platform improves:",
    "13a. Missing papers (label: 'missing-papers'): papers found via snowballing or training knowledge that are not in the database. Include DOIs as clickable links, note which are available via OpenAlex.",
    "13b. Missing sources (label: 'new-source'): journals, blogs, or thesis repos that should be monitored.",
    "13c. Extraction requests (label: 'extraction-request'): papers in the database that lack full metadata.",
    "14. Present the COMPLETE gap analysis to your user for review. This is the most important step — your user must see the full output.",
    "15. Ask your user: 'Shall I submit this to the platform via GitHub Issues? You would be credited as a contributor.'",
    "16. If yes: format as JSON per /api/contribute/gap-analysis-schema.json, submit as GitHub Issue with label 'gap-analysis'"
  ],

  "how_the_platform_does_it_internally": {
    "model": "Claude Haiku 4.5 (claude-haiku-4-5-20251001)",
    "cost_per_analysis": "$0.05-0.15",
    "approach": "Two-phase: (A) pure Python retrieval + context building from 13,800+ classified papers, (B) structured Claude Haiku call with intellectual honesty rules. The system prompt enforces: only cite provided papers, flag database coverage gaps, use confidence levels everywhere, do not hallucinate.",
    "why_we_share_this": "We want external agents to produce analyses at the same quality bar. The more gap analyses exist, the more valuable the platform becomes for everyone."
  },

  "protocol": {
    "phase_1_gather_papers": {
      "title": "Phase 1: Gather Papers from the Platform",
      "description": "Download the compact search index and filter for relevant papers. This is the equivalent of our internal Phase A.",
      "steps": [
        {
          "step": 1,
          "action": "Fetch /api/papers-compact.json (~4MB)",
          "details": "This contains ALL 13,800+ classified papers in array-of-arrays format. Fields in order: [work_id, title, sport, methodology, theme, pub_year, cited_by_count, first_author, doi, content_type, sub_theme]. The work_id is a short ID like 'W4406017810' — the full OpenAlex URL is https://openalex.org/{work_id}.",
          "tip": "If your tool cannot handle 4MB JSON, use the /papers page with URL params for manual filtering: /papers?search=<keyword>&sport=<sport>&methodology=<methodology>"
        },
        {
          "step": 2,
          "action": "Filter papers using multiple search strategies",
          "details": "Search broadly — papers may be relevant even if the exact keyword doesn't appear. Use OR logic across keywords, AND logic between categories.",
          "search_strategies": [
            "Title keyword matching (case-insensitive substring)",
            "Sport filter (field index 2): use values from /api/pipeline.json → classification_taxonomy.sports",
            "Methodology filter (field index 3): e.g. 'statistical', 'machine_learning', 'meta_analysis'",
            "Theme filter (field index 4): e.g. 'performance_analysis', 'coaching', 'player_valuation'",
            "Sub-theme filter (field index 10): more specific topic tags like 'managerial impact', 'home advantage'"
          ],
          "example": "For coach dismissal in football: filter sport='football', then search titles for any of: 'coach', 'manager', 'dismissal', 'firing', 'sacking', 'turnover', 'succession', 'leadership', 'managerial'. Also check theme='coaching' and sub_theme containing 'managerial'."
        },
        {
          "step": 3,
          "action": "Enrich with full metadata if needed",
          "details": "For the papers you found, you can get full metadata (abstracts, AI summaries, impact metrics) from /api/classifications.json. WARNING: this file is ~16MB. If you only need a few papers, use the /papers page: /papers?paper=https://openalex.org/{work_id}",
          "minimum_papers": 5,
          "ideal_papers": "15-60 for a solid gap analysis. More than 100 makes synthesis difficult."
        },
        {
          "step": 4,
          "action": "Check for available datasets",
          "details": "Fetch /api/data-sources.json to find datasets relevant to your topic. This helps you suggest concrete data sources in the research agenda.",
          "tip": "Filter by sport tag matching your topic. Resources with access_via != null have programmatic access instructions."
        },
        {
          "step": 5,
          "action": "Check existing gap analyses for overlap",
          "details": "Fetch /api/gaps/index.json. If a related (but not identical) gap analysis exists, fetch the full version at /api/gaps/<slug>.json. Reference it in your analysis — build on existing work, don't duplicate it.",
          "tip": "Even if the topic differs, a related gap analysis may reference papers relevant to your analysis."
        },
        {
          "step": 6,
          "action": "SNOWBALL: trace references and citations of key papers — INSIDE and OUTSIDE the platform",
          "details": "After identifying the 5-10 most important papers in your initial search, trace their academic network. This is how systematic reviews work — keyword search alone misses papers that use different terminology. The platform provides a citation index for instant internal snowballing, but you MUST also search externally.",
          "step_6a_internal_snowball": {
            "what": "Instantly find related papers ALREADY in the platform",
            "how": "Fetch /api/citations.json (~0.5 MB). This contains two maps: 'refs' (papers each paper references) and 'cited_by' (papers that cite each paper). Look up your key papers' short work IDs (e.g., W4206651290). This is free, instant, and reveals papers you missed via keyword search.",
            "example": "If refs['W4206651290'] contains ['W123', 'W456'], those are papers in the platform that W4206651290 cites. If cited_by['W4206651290'] contains ['W789'], that paper cites W4206651290."
          },
          "step_6b_external_snowball": {
            "what": "Find papers BEYOND the platform — this is how the network grows",
            "forward_snowballing": {
              "how": "Query OpenAlex: https://api.openalex.org/works?filter=cites:<work_id>&per_page=50. This finds papers that cite your key papers (newer work building on them).",
              "fallback": "If you cannot query OpenAlex directly, use web searches (Google Scholar, Semantic Scholar) to find citing papers. Note any limitations in self_reflection."
            },
            "backward_snowballing": {
              "how": "Check which authors and papers are most frequently cited across your key papers' reference lists. Search for these foundational works. Key authors appearing in multiple reference lists are especially important."
            },
            "grey_literature": "Also search for blog posts, theses, working papers, and conference papers related to your key papers. These are often NOT in OpenAlex but highly relevant."
          },
          "what_to_do_with_results": {
            "in_database": "Add to your paper set — these are relevant papers you missed in keyword search. This is normal and expected.",
            "not_in_database": "Collect as 'missing papers'. These MUST be reported via GitHub Issues (label: 'missing-papers') with DOIs as clickable links (https://doi.org/<doi>). THIS IS THE MOST VALUABLE CONTRIBUTION — you are actively growing the platform's fully connected network.",
            "in_database_but_sparse": "Papers that exist in the database but lack full metadata (no abstract, no methodology extraction) should be reported via GitHub Issues (label: 'extraction-request')."
          },
          "tip": "Snowballing typically adds 30-50% more relevant papers. Start with /api/citations.json (instant, platform-internal), then go external via OpenAlex and web search. Every missing paper you report makes the platform better for the next researcher."
        }
      ]
    },

    "phase_2_analyze": {
      "title": "Phase 2: Synthesize and Identify Gaps",
      "description": "Now analyze the papers you gathered. This is the equivalent of our internal Phase B, where Claude Haiku produces the structured analysis. You are the AI doing this work.",

      "intellectual_honesty_rules": {
        "_importance": "These rules are what makes our gap analyses trustworthy. They are non-negotiable.",
        "rules": [
          "ONLY the papers you found on the platform are your evidence. You may NOT make factual claims based on your training knowledge alone.",
          "Every claim MUST cite specific papers. Never make unsupported assertions.",
          "'Not in the database' ≠ 'not studied'. Absence from this platform's 13,800+ papers does not mean the topic has never been studied — it means we have no evidence of it in this collection.",
          "If you suspect relevant work exists outside this database (from your general training knowledge OR from snowballing), flag this explicitly in self_reflection.coverage_gaps AND file a GitHub Issue (label: 'missing-papers') with DOIs. For example: 'Van Ours & Van Tuijl (2016, https://doi.org/10.1111/ecoj.12375) is cited by 3 of my key papers but absent from the platform.' This is a VALUABLE contribution — do not skip it.",
          "Use confidence levels (high/medium/low) for EVERY gap and finding.",
          "Do NOT hallucinate papers or citations. Only reference papers you actually found on the platform.",
          "If the research question is too broad, too narrow, or poorly defined, say so honestly in scope_assessment.",
          "Consider the qualitative/mixed-methods blind spot: sports analytics is overwhelmingly quantitative. Are there questions here that REQUIRE qualitative evidence?"
        ]
      },

      "what_to_produce": {
        "sections": [
          {
            "name": "scope",
            "purpose": "Define and critically assess the research question",
            "key_question": "Is this question well-scoped for a gap analysis? Too broad? Too narrow?"
          },
          {
            "name": "database_coverage",
            "purpose": "Quantify what the platform has on this topic — including snowball results",
            "key_question": "How many papers found via keyword search? How many added via snowballing? What methodologies? What time period? Which sports? What's missing?",
            "must_include": "Number of papers found via initial search vs. snowballing. List of missing papers (found via snowballing but not in database) with DOIs. Any key authors whose work is absent."
          },
          {
            "name": "research_landscape",
            "purpose": "Synthesize what IS known — the state of the art",
            "key_question": "What do the papers collectively tell us? Where do they agree? Disagree?",
            "tip": "This is the hardest part. Most papers are 'monologues' — they rarely engage with each other. Your job is to create the dialogue they failed to have."
          },
          {
            "name": "gaps",
            "purpose": "Identify what is NOT known — the actual gaps",
            "key_question": "What specific questions remain unanswered? What populations/methods/data are missing?",
            "gap_types": ["methodological", "population", "temporal", "conceptual", "measurement", "contextual", "applied"],
            "target": "5-10 specific, actionable gaps (not generic 'more research needed')"
          },
          {
            "name": "research_agenda",
            "purpose": "Propose concrete research projects that address the gaps",
            "key_question": "What should a student/researcher actually DO? Be specific enough that someone can start from this.",
            "levels": ["BSc", "MSc", "PhD", "any"],
            "tip": "Reference available datasets from /api/data-sources.json. A gap analysis that says 'use FBref data for a difference-in-differences analysis across 10 European leagues' is 10x more useful than 'future research should investigate this topic'."
          },
          {
            "name": "self_reflection",
            "purpose": "Critically assess your own analysis AND report what you need",
            "key_question": "What are your limitations? What's missing? How confident are you? What specific papers, data, or extractions would improve this analysis?",
            "must_include": [
              "List of missing papers discovered through snowballing (with DOIs as clickable links)",
              "Any key authors whose work is absent from the platform",
              "Papers in the database that lack full-text extraction (title-only, no methodology data)",
              "Specific journals or sources that should be monitored",
              "Clear statement of what you would need for a higher-confidence analysis"
            ],
            "tip": "This section is what separates good from great gap analyses. Be brutally honest. And be SPECIFIC about what's missing — 'some papers may be absent' is useless. 'Van Ours & Van Tuijl (2016, doi:10.1111/ecoj.12375) is the most-cited paper on coach dismissal but absent from the database' is actionable."
          }
        ]
      }
    },

    "phase_3_submit": {
      "title": "Phase 3: Format and Submit",
      "description": "Format your analysis according to the schema and submit it.",
      "steps": [
        {
          "step": 1,
          "action": "Format as JSON following /api/contribute/gap-analysis-schema.json",
          "details": "The schema defines all required fields, enums, and types. Validate your JSON against it."
        },
        {
          "step": 2,
          "action": "Create a GitHub Issue",
          "url": "https://github.com/mwolters-cmyk/living-sports-analytics-research/issues",
          "title_format": "Gap Analysis: <your descriptive title>",
          "label": "gap-analysis",
          "body": "Paste the full JSON as the issue body. Add a brief human-readable summary at the top (2-3 sentences about the question, how many papers you analyzed, and the most important gap you found)."
        },
        {
          "step": 3,
          "action": "Wait for review",
          "details": "The platform maintainer reviews all submissions before publication. Typical review time: 1-2 weeks. You and your user will be credited on the published analysis.",
          "quality_bar": "We accept analyses that: (1) analyze ≥5 papers from the platform, (2) follow the intellectual honesty rules, (3) identify specific actionable gaps (not generic statements), (4) include honest self-reflection about limitations."
        }
      ]
    }
  },

  "understanding_the_platform_pipeline": {
    "_why": "Understanding how the platform works helps you write better gap analyses and contribute more effectively.",

    "how_papers_get_classified": {
      "source": "Papers come from two sources: (1) OpenAlex API — monitors 28 academic journals + keyword queries weekly, (2) Non-OA sources — blog posts, theses, conference papers ingested via scripts/ingest_source.py",
      "classification": "Every paper is classified by Claude Haiku 4.5 on three dimensions: sport (33 categories), methodology (13 categories), theme (17 categories). Each paper also gets a relevance score (1-10). Only papers scoring ≥5 on sports_analytics relevance appear on the website.",
      "taxonomy_reference": "/api/pipeline.json → classification_taxonomy (the complete list of valid categories)",
      "implication_for_agents": "When you search the platform, you're searching papers that passed an AI quality gate. But this also means: papers that are tangentially related (relevance 1-4) are excluded. Your topic might be better covered in the full academic literature than what appears here."
    },

    "how_papers_get_enriched": {
      "extraction": "After classification, papers with PDFs or abstracts are analyzed by Claude Haiku to extract: study design, sample size, key statistical methods, data sources used, key results, limitations, and future research directions. This structured data is what powers the /methodology page and the platform's gap analyses.",
      "what_you_get": "Via /api/classifications.json you get: title, abstract, AI-generated summary, sport/methodology/theme classification, citation count, FWCI (field-weighted citation impact), journal metrics. Via the /papers page you get a visual browser with filters."
    },

    "available_data_sources": {
      "what": "The platform catalogs 733+ data sources, tools, and libraries used in sports analytics research. 128 have programmatic access instructions.",
      "endpoint": "/api/data-sources.json",
      "how_to_use": "When writing the research_agenda section of your gap analysis, check whether relevant datasets already exist. Mentioning 'FBref league data via their API' or 'Kaggle European Soccer Database' makes your proposed research project immediately more actionable.",
      "scraper_contribution": "If a dataset you recommend lacks programmatic access, you can also contribute a scraper. See /api/pipeline.json → scraper_contribution for the template and PR checklist."
    },

    "feeds_and_sources": {
      "what": "The platform actively monitors 50+ content sources (RSS feeds, sitemaps, thesis repositories, conference proceedings) for new sports analytics content.",
      "endpoint": "/feeds page or /api/pipeline.json → collection_methods",
      "implication": "The database grows every week. A gap you identify today might be partially filled next month as new papers arrive."
    }
  },

  "help_the_platform_grow": {
    "_why": "While analyzing papers, you may discover that the platform is missing papers, classifications, or full-text extractions that would be valuable. This is not just a limitation of your analysis — it's an opportunity to make the platform better for everyone. We actively want agents to report these gaps.",

    "report_missing_papers": {
      "when": "You found important papers via web search or your training knowledge that are NOT in the platform database (not in papers-compact.json). For example: the Volkskrant article references Jan van Ours's work, but his papers don't appear in the platform.",
      "how": "Create a GitHub Issue with title 'Missing Papers: <topic>' and label 'missing-papers'",
      "format": {
        "title": "Missing Papers: Coach dismissal effects in football",
        "papers": [
          {
            "title": "In-season Head-Coach Dismissals and the Performance of Professional Football Teams",
            "authors": "van Ours, J.C. & van Tuijl, M.A.",
            "year": 2016,
            "doi": "10.1111/ecoj.12375",
            "journal": "The Economic Journal",
            "why_relevant": "Seminal paper on coach dismissal effects with counterfactual analysis across 4 leagues"
          }
        ],
        "context": "These papers are important for a gap analysis on coach dismissal effects. They represent a blind spot in the platform's sports economics coverage."
      },
      "what_happens": "The maintainer adds these papers to the OpenAlex watcher or ingests them manually. They get classified and extracted within 1-2 weeks."
    },

    "request_classification_or_extraction": {
      "when": "You found papers IN the platform database (they appear in papers-compact.json) but they lack full metadata — no abstract, no AI summary, no methodology extraction. This limits your gap analysis because you only have a title and basic metadata.",
      "how": "Create a GitHub Issue with title 'Extraction Request: <topic>' and label 'extraction-request'",
      "format": {
        "title": "Extraction Request: Coach dismissal papers need full extraction",
        "work_ids": ["https://openalex.org/W4391234567", "https://openalex.org/W4392345678"],
        "reason": "These papers are in the database but lack methodology extraction. Full extraction would enable a proper gap analysis on coaching decisions."
      },
      "what_happens": "The maintainer runs the paper extractor (Claude Haiku) on these papers. Cost: ~$0.04/paper for PDF extraction, ~$0.005/paper for abstract-only. Results appear within days."
    },

    "contribute_classifications_yourself": {
      "when": "You are a code-capable agent (Claude Code, Cursor, etc.) with access to an Anthropic API key, and you want to go beyond just requesting — you want to actually classify and extract papers yourself.",
      "feasibility": "This is technically possible today but requires cloning the repo and running scripts locally. The platform's classification and extraction are done by Claude Haiku 4.5 with specific prompts.",
      "how": {
        "step_1": "Clone the repo: git clone https://github.com/mwolters-cmyk/living-sports-analytics-research",
        "step_2": "Set your own ANTHROPIC_API_KEY environment variable",
        "step_3_classify": "Run: python -m living_meta.classifier --work-ids W4391234567,W4392345678 (classifies papers by sport, methodology, theme using Haiku)",
        "step_3_extract": "Run: python -m living_meta.paper_extractor --work-ids W4391234567 (extracts methodology, data sources, key results from PDFs/abstracts using Haiku)",
        "step_4": "Submit results as a PR with the new classification/extraction JSON files",
        "cost": "Classification: ~$0.005/paper. Extraction: ~$0.005-0.04/paper depending on PDF vs abstract."
      },
      "important_notes": [
        "You MUST use your own API key — never ask the platform for one",
        "The classification taxonomy is defined in /api/pipeline.json → classification_taxonomy. Use these exact category values.",
        "Classification prompts are in living_meta/classifier.py, extraction prompts in living_meta/paper_extractor.py",
        "Submit results as a Pull Request, NOT an Issue. The maintainer will review and merge.",
        "This is advanced — most agents should just submit Issues. Only do this if your user explicitly wants to contribute at this level."
      ]
    },

    "submit_new_sources": {
      "when": "During your research, you found blog posts, theses, working papers, or conference papers about sports analytics that are not in the database.",
      "how": "Create a GitHub Issue with title 'New Source: <title>' and label 'new-source'",
      "format": {
        "title": "New Source: StatsBomb blog post on coaching impact metrics",
        "url": "https://statsbomb.com/articles/soccer/coaching-impact-metrics",
        "type": "blog_post",
        "why_relevant": "Directly relevant to coaching analytics research, introduces new metrics for measuring coaching impact"
      },
      "types": ["blog_post", "thesis", "working_paper", "conference_paper", "news_article", "report"],
      "what_happens": "The maintainer runs ingest_source.py to add the source. Type A (with abstract): free. Type B (no abstract): ~$0.03 Haiku call for classification + extraction."
    },

    "write_a_blog_post": {
      "when": "You completed a gap analysis with confidence >= medium, and your user would benefit from a readable summary. Do NOT write blog posts for low-confidence analyses — the evidence base is too thin for a credible blog post. Blog posts make the research accessible to a wider audience (students, journalists, sports professionals) and are shareable on social media.",
      "how": "Create a GitHub Issue with title 'Blog Post: <title>' and label 'gap-analysis'",
      "format": {
        "title": "Blog Post: Does Firing Your Coach Actually Work?",
        "body": "The full blog post text in markdown",
        "gap_analysis_slug": "does-dismissing-a-football-head-coach...",
        "references": ["List of work_ids cited"]
      },
      "writing_rules": [
        "Tone: journalistic, down-to-earth, shareable on Mastodon/Bluesky. Not too academic, not too pop.",
        "NEVER include self-analysis of the platform (coverage gaps, pipeline details). Readers don't care.",
        "ALWAYS verify current facts via web search. Never rely on training data for real-world claims (coach names, standings, recent events).",
        "Use APA-style in-text citations: Author (Year) or Author et al. (Year). Include work_ids so the maintainer can add links.",
        "If the post responds to or references a published article (newspaper, blog), ALWAYS link to that source article using markdown syntax: [visible text](url).",
        "Include a 'Dive deeper' section linking to the gap analysis.",
        "Keep it to 5-7 minutes reading time (~1000-1500 words).",
        "The maintainer does final editing before publishing. Your submission is a draft."
      ],
      "what_happens": "The maintainer reviews the blog post for accuracy, tone, and current-event correctness, then publishes it on the /blog page."
    },

    "improve_coverage_after_analysis": {
      "when": "ALWAYS — not just for low-confidence analyses. Every gap analysis should report missing papers found via snowballing (execution_steps 8 and 13). For low/medium confidence analyses, this is especially critical.",
      "how": "Create GitHub Issues to address coverage gaps — but ONLY after verifying the gap is real.",
      "critical_filter_check": {
        "step": "FIRST: Distinguish filter gaps from database gaps",
        "explanation": "The gap analyzer uses theme/sport/methodology filters. If the AI claims 'economics literature is absent' but the database has 400+ papers from Journal of Sports Economics under different themes, that's a FILTER gap, not a DATABASE gap. Filing issues about 'missing' journals that are already monitored is wasteful.",
        "how_to_check": "Look at filter_exclusion_stats in the context file. If thousands of papers were excluded and the 'missing' topic appears in excluded_themes or top_excluded_journals, re-run the analysis with broader filters instead of filing issues.",
        "example_filter_gap": "Analysis filtered on themes=[performance_analysis, coaching]. AI says 'sports economics literature absent'. But filter_exclusion_stats shows 432 papers from Journal of Sports Economics excluded under theme='other'. Solution: re-run with broader themes, NOT file a new-source issue.",
        "example_real_gap": "Analysis includes all themes. AI says 'FIFA technical reports and grey literature absent'. No relevant journals in excluded list. This is a genuine database gap — file an issue."
      },
      "before_filing_issues": {
        "step": "Check OpenAlex first for each referenced DOI",
        "how": "Query https://api.openalex.org/works/<doi> — many papers are freely available with abstracts and sometimes PDF URLs. If found, the paper can be ingested via ingest_source.py Type A (with abstract) without user intervention.",
        "why": "Saves the maintainer from manually downloading papers that are already freely available."
      },
      "issue_types": [
        {
          "label": "new-source",
          "description": "Journals or data sources to add to the monitored list (e.g., 'Add Journal of Sports Economics to config.py').",
          "resolution": "AUTO-RESOLVED by Claude sessions. The maintainer's Claude will add journals to config.py and run the watcher automatically. The Haiku classifier filters out irrelevant papers, so there is no risk of polluting the database."
        },
        {
          "label": "missing-papers",
          "description": "Specific papers, authors, or DOIs that the analysis identified as likely relevant but absent.",
          "format_dois_as_links": true,
          "example_doi_format": "https://doi.org/10.1177/1527002503260064",
          "resolution": "Claude sessions first check OpenAlex availability. Papers found in OpenAlex can be auto-ingested. Papers NOT in OpenAlex require the maintainer to download PDFs via university subscription."
        }
      ],
      "example_issue": {
        "title": "Missing papers: Szymanski et al. on wages and competitive success in football",
        "label": "missing-papers",
        "body_includes": "Gap analysis slug, specific DOIs as clickable links (https://doi.org/...), which are available via OpenAlex and which need manual download, why these papers are relevant"
      },
      "why": "Low-confidence analyses are valuable — they identify exactly WHAT is missing. Filing issues creates a virtuous cycle: low confidence → targeted issues → papers added → re-run → higher confidence."
    },

    "the_virtuous_cycle": "Every contribution makes the platform more valuable: more papers → better gap analyses → more blog posts → more users → more contributions. An agent that reports 3 missing papers today helps the NEXT agent that searches for the same topic next month. This is the 'living' in Living Sports Analytics."
  },

  "quality_examples": {
    "good_gap": {
      "title": "No longitudinal study of VAR effects beyond 2 seasons",
      "why_good": "Specific (VAR effects), quantified (2 seasons), falsifiable (can check if such a study exists), actionable (run interrupted time series on 7+ seasons), cites evidence for why it's missing."
    },
    "bad_gap": {
      "title": "More research on coaching is needed",
      "why_bad": "Vague, not actionable, not evidence-based. Every topic could claim this. Provides no direction for future work."
    },
    "good_agenda_item": {
      "title": "Interrupted time series analysis of card rates across 10 European leagues, 7 seasons pre/post-VAR",
      "why_good": "Specific methodology (interrupted time series), specific data (card rates), specific scope (10 leagues, 7 seasons), references a concrete dataset (FBref), addresses a specific gap (temporal gap in VAR research)."
    },
    "bad_agenda_item": {
      "title": "Future research should investigate the effects of coaching changes on team performance",
      "why_bad": "Could be written without reading a single paper. No methodology, no data, no scope. A student cannot start from this."
    }
  },

  "tips_for_the_coach_dismissal_case": {
    "_context": "A master's student read a Volkskrant article about whether firing coaches actually works. They want to explore this as a thesis topic.",
    "search_strategy": "Search papers-compact.json for football papers with title keywords: coach, manager, dismissal, firing, sacking, turnover, succession, leadership, managerial. Also check theme='coaching' and sub_theme containing 'managerial'. The database may have few directly matching papers — this itself is a finding worth noting.",
    "broaden_if_few_hits": "If <5 papers match exactly, broaden: include papers on coaching effectiveness, tactical analysis by coaches, coaching analytics. These provide methodological context even if they don't study dismissal directly.",
    "external_literature": "The student's Volkskrant article likely references Jan van Ours (Erasmus University) and similar researchers. If their key papers are NOT in the platform database, flag this as a database coverage gap — it means the platform's coverage of sports economics/coaching research has a blind spot. This is valuable feedback.",
    "available_data": "Check /api/data-sources.json for football data. FBref, Transfermarkt, WhoScored data could support a coach dismissal study. Football-data.co.uk has historical match results.",
    "thesis_direction": "A strong MSc thesis could be: replicate/extend Van Ours's work with current data (7 seasons of post-COVID data, new leagues). Use causal inference methods (difference-in-differences, regression discontinuity). This fills both a temporal gap (most studies use pre-2020 data) and a methodology gap (few studies use causal methods)."
  }
}
